{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737f6dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f586e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.13.1)\n",
      "Collecting tensorflow-intel==2.13.1 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.1 from https://files.pythonhosted.org/packages/6b/f8/3dc773923f4afa73b894ba25f83f8a0f1a549783ab99b496e0f5d0b86099/tensorflow_intel-2.13.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.13.1-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for gast<=0.4.0,>=0.2.1 from https://files.pythonhosted.org/packages/b6/48/583c032b79ae5b3daa02225a675aeb673e58d2cb698e78510feceb11958c/gast-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (3.9.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/0b/2d/3f480b1e1d31eb3d6de5e3ef641954e5c67430d5ac93b7fa7e07589576c7/libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/ad/6e/1bed3b7c904cc178cb8ee8dbaf72934964452b3de95b7a63412591edb93c/protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for typing-extensions<4.6.0,>=3.6.6 from https://files.pythonhosted.org/packages/31/25/5abcd82372d3d4a3932e1fa8c3dbf9efac10cc7c0d16e78467460571b404/typing_extensions-4.5.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.1->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/78/a9/eaa378e6fe421c2f61bdd4b92439b2b8bb320526f2b0e08fcf4e21c2f855/grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard<2.14,>=2.13 from https://files.pythonhosted.org/packages/67/f2/e8be5599634ff063fa2c59b7b51636815909d5140a26df9f02ce5d99b81a/tensorboard-2.13.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-io-gcs-filesystem>=0.23.1 from https://files.pythonhosted.org/packages/ac/4e/9566a313927be582ca99455a9523a097c7888fc819695bdc08415432b202/tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.1->tensorflow) (0.38.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9e/8d/ddbcf81ec751d8ee5fd18ac11ff38a0e110f39dfbf105e6d9db69d556dd0/google_auth-2.29.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for google-auth-oauthlib<1.1,>=0.5 from https://files.pythonhosted.org/packages/4a/07/8d9a8186e6768b55dfffeb57c719bc03770cf8a970a074616ae6f9e26a57/google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/7a/13/e503968fefabd4c6b2650af21e110aa8466fe21432cd7c43a84577a89438/tensorboard_data_server-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/fb/2b/a64c2d25a37aeb921fddb929111413049fc5f8b9a4c1aefaffaafe768d54/cachetools-5.3.3-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for rsa<5,>=3.1.4 from https://files.pythonhosted.org/packages/49/97/fa78e3d2f65c02c8e1268b9aba606569fe97f6c8f7c2d74394553347c145/rsa-4.9-py3-none-any.whl.metadata\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for requests-oauthlib>=0.7.0 from https://files.pythonhosted.org/packages/3b/5d/63d4ae3b9daea098d5d6f5da83984853c1bbacd5dc826764b249fe119d24/requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.1->tensorflow)\n",
      "  Obtaining dependency information for oauthlib>=3.0.0 from https://files.pythonhosted.org/packages/7e/80/cab10959dc1faead58dc8384a781dfbf93cb4d33d50988f7a69f1b7c9bbe/oauthlib-3.2.2-py3-none-any.whl.metadata\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow_intel-2.13.1-cp311-cp311-win_amd64.whl (276.6 MB)\n",
      "   ---------------------------------------- 0.0/276.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.8/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 8.4/276.6 MB 88.8 MB/s eta 0:00:04\n",
      "   - ------------------------------------- 13.1/276.6 MB 108.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------ 18.2/276.6 MB 108.8 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 20.7/276.6 MB 81.8 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 23.5/276.6 MB 73.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 26.3/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 29.2/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 32.1/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 35.1/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 38.1/276.6 MB 59.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 41.3/276.6 MB 65.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 44.8/276.6 MB 65.6 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 48.1/276.6 MB 65.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 51.6/276.6 MB 73.1 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 54.1/276.6 MB 65.6 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 56.4/276.6 MB 59.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 59.0/276.6 MB 54.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 61.0/276.6 MB 50.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 63.0/276.6 MB 46.7 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 65.1/276.6 MB 46.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 67.2/276.6 MB 43.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 69.4/276.6 MB 43.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 71.5/276.6 MB 43.7 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 73.1/276.6 MB 40.9 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 74.7/276.6 MB 40.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 76.3/276.6 MB 40.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 78.1/276.6 MB 40.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 79.7/276.6 MB 38.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 81.2/276.6 MB 36.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 82.2/276.6 MB 34.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 83.3/276.6 MB 32.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 84.4/276.6 MB 31.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 85.6/276.6 MB 29.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 86.8/276.6 MB 28.5 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 88.2/276.6 MB 27.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 89.4/276.6 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 90.6/276.6 MB 26.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 91.6/276.6 MB 25.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 92.7/276.6 MB 25.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 93.9/276.6 MB 25.2 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 95.2/276.6 MB 26.2 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 96.7/276.6 MB 27.3 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 97.9/276.6 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 98.9/276.6 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 99.6/276.6 MB 25.2 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 99.8/276.6 MB 22.5 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 100.9/276.6 MB 22.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 102.3/276.6 MB 22.6 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 103.8/276.6 MB 23.4 MB/s eta 0:00:08\n",
      "   -------------- ------------------------ 105.2/276.6 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 106.8/276.6 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------- ----------------------- 108.3/276.6 MB 24.3 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 109.6/276.6 MB 25.2 MB/s eta 0:00:07\n",
      "   --------------- ----------------------- 110.7/276.6 MB 28.5 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 112.1/276.6 MB 28.5 MB/s eta 0:00:06\n",
      "   --------------- ----------------------- 113.4/276.6 MB 28.5 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 114.9/276.6 MB 28.4 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 116.4/276.6 MB 28.4 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 117.5/276.6 MB 28.4 MB/s eta 0:00:06\n",
      "   ---------------- ---------------------- 118.2/276.6 MB 26.2 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 119.0/276.6 MB 25.2 MB/s eta 0:00:07\n",
      "   ---------------- ---------------------- 119.9/276.6 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 120.9/276.6 MB 24.2 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 122.0/276.6 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 123.2/276.6 MB 23.4 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 124.3/276.6 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 125.6/276.6 MB 22.5 MB/s eta 0:00:07\n",
      "   ----------------- --------------------- 127.1/276.6 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 128.6/276.6 MB 24.2 MB/s eta 0:00:07\n",
      "   ------------------ -------------------- 130.1/276.6 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 131.4/276.6 MB 27.3 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 132.5/276.6 MB 28.5 MB/s eta 0:00:06\n",
      "   ------------------ -------------------- 133.7/276.6 MB 28.4 MB/s eta 0:00:06\n",
      "   ------------------- ------------------- 135.1/276.6 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 136.5/276.6 MB 28.4 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 137.9/276.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 139.3/276.6 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 140.4/276.6 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------------- ------------------- 141.6/276.6 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 142.7/276.6 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 144.1/276.6 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 145.4/276.6 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 146.3/276.6 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------------- ------------------ 146.9/276.6 MB 24.3 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 147.4/276.6 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 148.1/276.6 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------------- ------------------ 148.8/276.6 MB 20.5 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 149.4/276.6 MB 19.8 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 150.0/276.6 MB 18.7 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 150.7/276.6 MB 18.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 151.4/276.6 MB 17.7 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 152.0/276.6 MB 16.8 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 152.6/276.6 MB 16.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 153.3/276.6 MB 16.0 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 154.1/276.6 MB 15.6 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 154.9/276.6 MB 14.9 MB/s eta 0:00:09\n",
      "   --------------------- ----------------- 155.9/276.6 MB 14.9 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 156.9/276.6 MB 15.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 158.0/276.6 MB 16.0 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 158.1/276.6 MB 15.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 158.3/276.6 MB 14.6 MB/s eta 0:00:09\n",
      "   ---------------------- ---------------- 159.2/276.6 MB 14.9 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 160.0/276.6 MB 15.2 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 161.0/276.6 MB 15.6 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 161.9/276.6 MB 16.4 MB/s eta 0:00:08\n",
      "   ---------------------- ---------------- 163.0/276.6 MB 17.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 164.1/276.6 MB 17.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 165.1/276.6 MB 18.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 166.0/276.6 MB 18.2 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 167.0/276.6 MB 17.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 168.1/276.6 MB 17.7 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 169.2/276.6 MB 21.1 MB/s eta 0:00:06\n",
      "   ------------------------ -------------- 170.3/276.6 MB 22.6 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 171.6/276.6 MB 23.4 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 173.0/276.6 MB 24.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 174.5/276.6 MB 25.2 MB/s eta 0:00:05\n",
      "   ------------------------ -------------- 175.9/276.6 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 177.4/276.6 MB 28.5 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 178.9/276.6 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 180.0/276.6 MB 29.7 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 180.8/276.6 MB 28.4 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 181.7/276.6 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 182.6/276.6 MB 26.2 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 183.7/276.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 184.8/276.6 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 186.0/276.6 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 187.4/276.6 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 188.9/276.6 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------------- ------------ 190.4/276.6 MB 25.2 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 191.5/276.6 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 192.6/276.6 MB 27.3 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 193.7/276.6 MB 27.3 MB/s eta 0:00:04\n",
      "   --------------------------- ----------- 194.9/276.6 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 196.3/276.6 MB 28.5 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 197.3/276.6 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 198.4/276.6 MB 26.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 199.4/276.6 MB 25.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 200.5/276.6 MB 24.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 201.6/276.6 MB 24.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ---------- 202.9/276.6 MB 25.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 204.3/276.6 MB 25.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 205.8/276.6 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 207.3/276.6 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 208.8/276.6 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 210.4/276.6 MB 29.7 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 212.0/276.6 MB 32.7 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 213.9/276.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 215.5/276.6 MB 32.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 217.5/276.6 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 219.6/276.6 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 221.6/276.6 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 223.1/276.6 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 224.6/276.6 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 225.8/276.6 MB 38.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 227.0/276.6 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 228.1/276.6 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 229.2/276.6 MB 31.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 230.6/276.6 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 231.9/276.6 MB 28.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 233.0/276.6 MB 27.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 234.2/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 235.3/276.6 MB 25.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 236.4/276.6 MB 25.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 237.8/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 239.1/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 240.4/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 241.7/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 242.9/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 243.9/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 245.0/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 246.4/276.6 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 247.3/276.6 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 248.4/276.6 MB 25.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 249.5/276.6 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 250.6/276.6 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 251.8/276.6 MB 24.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 253.2/276.6 MB 24.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 254.5/276.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 255.9/276.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 257.5/276.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 259.0/276.6 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 260.1/276.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 261.2/276.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 262.5/276.6 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 263.5/276.6 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 264.2/276.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 264.9/276.6 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 265.5/276.6 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.0/276.6 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 266.6/276.6 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 267.3/276.6 MB 19.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 268.0/276.6 MB 18.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 268.9/276.6 MB 18.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  269.9/276.6 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  270.8/276.6 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  271.5/276.6 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/276.6 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  273.7/276.6 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  275.0/276.6 MB 17.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  275.9/276.6 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.6/276.6 MB 19.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 276.6/276.6 MB 7.7 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.8/3.8 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.0/3.8 MB 25.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 22.0 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 30.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.5/26.4 MB 31.2 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 2.9/26.4 MB 31.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 4.4/26.4 MB 31.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 5.7/26.4 MB 30.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 6.7/26.4 MB 28.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 7.9/26.4 MB 28.2 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 9.2/26.4 MB 28.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.6/26.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.1/26.4 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.3/26.4 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.1/26.4 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 14.9/26.4 MB 25.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 15.6/26.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.6/26.4 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.5/26.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.5/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.7/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.9/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 22.3/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.7/26.4 MB 22.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.2/26.4 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 21.8 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB ? eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 1.3/5.6 MB 40.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.4/5.6 MB 30.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.3/5.6 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.6/5.6 MB 26.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.6/5.6 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 25.4 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 0.0/440.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 440.8/440.8 kB 26.9 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 1.1/1.5 MB 36.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 23.8 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "   ---------------------------------------- 0.0/189.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 189.2/189.2 kB ? eta 0:00:00\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "   ---------------------------------------- 0.0/151.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 151.7/151.7 kB ? eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.0\n",
      "    Uninstalling protobuf-3.20.0:\n",
      "      Successfully uninstalled protobuf-3.20.0\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.29.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.62.1 keras-2.13.1 libclang-18.1.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\nehaparveenmohamm\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\nehaparveenmohamm\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\nehaparveenmohamm\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\nehaparveenmohamm\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.2.2 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a697930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT available\n",
      "tfGPU is NOT available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"C:\\\\testing\\\\Final_phonetics_T5.csv\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is NOT available\")\n",
    "    \n",
    "# Check if GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"tfGPU is available\")\n",
    "else:\n",
    "    print(\"tfGPU is NOT available\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")# Initialize lists to store formatted data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bf8803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  2427,  5983,\n",
      "           76,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([3342,  107,   18, 7400,  476,   18,   32,   32,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    26,   138,\n",
      "        13272,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([10729,    18, 13272,     1,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  3225,  9629,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([    3,    88,   107,    29,    18,    52, 10266,     1,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,  3258,\n",
      "            7,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 4171,  566,  553,   18, 7392,    7,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   107,   782,\n",
      "           63,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 454, 9195,   18,   29,   15,   15,    1,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  9230,  8735,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([6741,   18,  776,   15,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   102, 18074,\n",
      "        13847,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([6045, 9851,   18,   29,   15,   15,   18,  524,   89,   89,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1047,  1273,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([    3, 17670,    18,    52,    23,   107,     7,   107,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    23,  5514,\n",
      "          152,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([    3,    15,    15,    18,  4882, 11083,    18,    29,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  4766,   425,\n",
      "            9,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 1258,  107,   18,   40,   32,  106,   18,  122,    9,  107,    1,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  7043,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  3,   9, 210,  29,  18,  63,   9,   1,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   210,    35,\n",
      "            7,    88,  1725,     1,     0,     0,     0,     0,     0,     0]), 'labels': tensor([    3,   210,    76,   107,    29,    18,     7, 28848,    29,   599,\n",
      "          122,    61,     1,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1927,  2917,\n",
      "           23,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 1927,  107,   18, 1050,   15,   18,   29,   15,   15,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    17,    77,\n",
      "            9,     7,    88,     1,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,   17,   15,   15,   18, 8607,   18,    7,   88,  107,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10, 10878,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,    9,  107,   29,   18, 5999,  566,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10, 26943,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 272, 9851,  439,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1927,   152,\n",
      "            9,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 1927,   18, 5033,   18,   76,  107,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,   222,\n",
      "          127,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 4171, 4950,   18,   17,  127,    1,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   265,   440,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,    9,  107,   18, 5365, 8638,    1,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    51,  2975,\n",
      "           23,    52,    17,     2,    29,     1,     0,     0,     0,     0]), 'labels': tensor([ 954,  210,   52,   18, 6808,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    88,  5317,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  3,  15, 107, 157,  18, 235, 107,  52,   1,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,     9,   115,\n",
      "           26,    83,   521,   107,     1,     0,     0,     0,     0,     0]), 'labels': tensor([    3,     9,   107,   115,    18,   308, 10955,    18,   521,   107,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10, 18983,   603,\n",
      "        15416,     9,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 954,  107,   18,  776,   15,   18, 4148,  566,   18, 1258,  107,    1,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  3464,    89,\n",
      "         1468,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 454, 1343,   18,  371,    9,  107, 1725,    1,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1169,    52,\n",
      "           77,     9,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 1050,   15,   18,   60,   15,   18, 8607,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   860,     9,\n",
      "         2388,    32,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([6660,    9,  107,   18, 4111,  566,   18,   52,   32,  107,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1859,   152,\n",
      "           15,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 3534,   107,    18,   134, 14084,    18,    29,    15,   107,     1,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  4953,    32,\n",
      "         1655,    17,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,   29,   15,   15,   18,  157,   32,  107,   18, 1655,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1395,  2060,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([3396,  566,   18, 2338,   18, 2866,    1,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  9434,   115,\n",
      "            9,  5326,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 9434,  107,   18, 4882,  566,   18,   29,    9,  210,   89,    1,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  2738, 17237,\n",
      "          107,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([9710,   18,   29,   15,   15,   18,   52,   32,   32,   26,    1,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  4470,     9,\n",
      "          107,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 480, 9195,   18, 8607,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  1273,     9,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  27,   18,    7, 1024,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   588,    77,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 20, 107,  18,  60,  35,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   397,  1677,\n",
      "           63,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,  122,   88,   15,   18, 9195,  448,   18,  122,   88,   15,    1,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10, 20547,    51,\n",
      "         1598,  1106,     9,     1,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   41,    29,    61,   172,    32,    32,    18,    51,    32,    32,\n",
      "           18, 12725,  4170,    18,    26,     9,   107,     1,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   138,    75,\n",
      "         9361,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3,    9,  107,   40,   18,  134, 5080,   18, 1395,    1,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,  1211,\n",
      "            9,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([5693, 2990,   18,    9,  107,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   172,    32,\n",
      "          154,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  3, 172,  32, 107,  18,   9,  63,   1,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  7708,    32,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([493, 107,  40,  18,  40,  32, 107,   1,   0,   0,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  4243,     9,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([1491,  107,   18,  521,  107,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,     9,    26,\n",
      "         2754,    76,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  3,   9, 107,  26,  18,  51,   9, 107,  18,   7,  32,  32,   1,   0,\n",
      "          0,   0,   0,   0,   0,   0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  9275,    29,\n",
      "          588,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([1923,  107,   18,   17,   77,   18,   26, 7392,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,     9,\n",
      "          588,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([22885,    18,    26,  7392,     1,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,  6372,   402,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 9766, 13641,    18,   157,    32,    32,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,   526,   122,\n",
      "         1024,     1,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([1212,  122,   18, 1024,    1,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    51,  5615,\n",
      "           29,     9,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([  283, 26418,   134,    18,  8607,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,     9,\n",
      "            7,   603,     9,     1,     0,     0,     0,     0,     0,     0]), 'labels': tensor([22885,    18,  2338,    18,    51,     9,   107,     1,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10, 11374,    63,\n",
      "            1,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([ 954,  107,   18,  308, 5080,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}, {'input_ids': tensor([ 1193,  3027,   564,    12,   951,  1225, 30637,    10,    29,  4268,\n",
      "          603,    32,     1,     0,     0,     0,     0,     0,     0,     0]), 'labels': tensor([   3, 8607,   18,  254, 5080,   18,   51,   32,  107,    1,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store formatted data\n",
    "formatted_data = []\n",
    "# Set max length for padding\n",
    "max_length = 20\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    name = row[\"names\"]\n",
    "    phonetic = row[\"phonetics\"]\n",
    "    \n",
    "    # Format input-target pair\n",
    "    input_sequence = f\"Convert name to phonetic pronunciation:{name}\"\n",
    "    target_sequence = phonetic\n",
    "    \n",
    "    # Tokenize input and target sequences\n",
    "    input_tokens = tokenizer.encode(input_sequence, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[0]\n",
    "    target_tokens = tokenizer.encode(target_sequence, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")[0]\n",
    "    \n",
    "    # Add formatted data to the list\n",
    "    formatted_data.append({\"input_ids\": input_tokens, \"labels\": target_tokens})\n",
    "print(formatted_data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51124637",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nehaparveenmohamm\\AppData\\Roaming\\Python\\Python311\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39300' max='39300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39300/39300 12:38:06, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.617400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.463800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.420100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.380300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.319400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.278600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.223900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.215500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.191300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.196500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.186200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.156700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.148100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.135600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.115400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.125100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.114000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.124800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>0.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>0.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>0.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>0.105500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>0.110300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>0.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>0.089400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.097300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>0.098300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>0.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>0.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>0.094400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.086600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>0.077400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>0.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.091900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>0.075400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>0.080700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.086900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>0.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>0.085000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>0.072900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>0.079100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>0.083800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>0.088400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>0.089200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>0.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>0.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>0.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>0.068000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>0.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.078300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>0.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15700</td>\n",
       "      <td>0.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15900</td>\n",
       "      <td>0.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16100</td>\n",
       "      <td>0.071700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16300</td>\n",
       "      <td>0.080600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>0.065600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16700</td>\n",
       "      <td>0.068800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>0.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16900</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17100</td>\n",
       "      <td>0.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17300</td>\n",
       "      <td>0.066800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>0.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17700</td>\n",
       "      <td>0.071100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17900</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18100</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>0.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18300</td>\n",
       "      <td>0.075100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>0.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18700</td>\n",
       "      <td>0.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>0.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18900</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19100</td>\n",
       "      <td>0.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19300</td>\n",
       "      <td>0.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>0.061500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>0.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19700</td>\n",
       "      <td>0.056000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>0.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19900</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20100</td>\n",
       "      <td>0.064900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20300</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20700</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>0.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20900</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.057500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21100</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21300</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21700</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21900</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22100</td>\n",
       "      <td>0.054900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>0.058600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22300</td>\n",
       "      <td>0.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>0.051600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22700</td>\n",
       "      <td>0.051800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22900</td>\n",
       "      <td>0.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23100</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23300</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>0.052800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23700</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23900</td>\n",
       "      <td>0.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24100</td>\n",
       "      <td>0.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24300</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>0.040500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24700</td>\n",
       "      <td>0.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>0.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24900</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25100</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25300</td>\n",
       "      <td>0.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>0.047000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.051900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25700</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25900</td>\n",
       "      <td>0.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26100</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>0.046500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26300</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>0.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26700</td>\n",
       "      <td>0.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>0.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26900</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27100</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>0.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27300</td>\n",
       "      <td>0.042300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.043000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>0.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27700</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27900</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28100</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28300</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28700</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28900</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29100</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>0.033700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29300</td>\n",
       "      <td>0.039900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29700</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29900</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30100</td>\n",
       "      <td>0.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30300</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.030500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>0.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30700</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30900</td>\n",
       "      <td>0.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31100</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>0.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31300</td>\n",
       "      <td>0.032700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31700</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31900</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32100</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32300</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32700</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32900</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33100</td>\n",
       "      <td>0.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33300</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33700</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33900</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34100</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>0.022700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34300</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.023300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34700</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34900</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35100</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35300</td>\n",
       "      <td>0.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35700</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>0.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35900</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36100</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36300</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36700</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36900</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37100</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37300</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37700</td>\n",
       "      <td>0.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>0.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37900</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38100</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38300</td>\n",
       "      <td>0.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38700</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38900</td>\n",
       "      <td>0.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39100</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39300</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [82/82 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_data = formatted_data[:int(0.8 * len(formatted_data))]\n",
    "val_data = formatted_data[int(0.8 * len(formatted_data)):int(0.9 * len(formatted_data))]\n",
    "test_data = formatted_data[int(0.9 * len(formatted_data)):]\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=30,\n",
    "    per_device_eval_batch_size=30,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=60,\n",
    "    logging_steps=100,\n",
    "    logging_dir='C:\\\\testing\\\\logs',\n",
    "    output_dir=\"C:\\\\testing\\\\output\",\n",
    "    weight_decay=0.1,\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"C:\\\\testing\\\\fine_tuned_t5_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e0a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_path = \"C:\\\\testing\\\\fine_tuned_t5_model\"\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d63b148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: seer-sha\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "input_text = \"Convert name to phonetic pronunciation:Saoirse\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "# Decode the output tokens to text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Output:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6bee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be5af8-9531-4363-9e5b-47d708d4a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a91711-699d-44d9-a477-a7874327335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abb1bd-c5f5-4c2f-b56a-11bf983fd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55aa0f-89e3-4432-8edf-325fe02bfe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4314273-a8f2-4916-bb10-c7e90bc287ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4aae44-e9cd-4c84-88ef-51185bb71b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade torch torchvision torchaudio transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb8f0f-feec-412e-9f6c-86e60001ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4164c81-e4d0-42de-aff5-ff6c9b69e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ca24e-efb2-4bb1-b60b-4e92c098ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f309cc0-ec43-45c3-9043-cef0b9cb83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "# Decode predictions\n",
    "predictions = []\n",
    "for example in test_data:\n",
    "    input_ids = example[\"input_ids\"].unsqueeze(0)  # Add batch dimension\n",
    "    outputs = model.generate(input_ids=input_ids, max_length=20, num_beams=5, early_stopping=True)\n",
    "    predicted_sequence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predictions.append(predicted_sequence.lower())\n",
    "\n",
    "\n",
    "# Prepare references\n",
    "references = [tokenizer.decode(example[\"labels\"], skip_special_tokens=True).lower() for example in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2f9620-9b22-43c9-abbe-833a28a96310",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_data = []\n",
    "for i in range (len(predictions)):\n",
    "    new_dict = {\"predictions\":predictions[i],\n",
    "     \"references\":references[i]}\n",
    "    f_data.append(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29a30ba8-00e1-4bc5-a69a-66e0a13dc9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': 'peeks', 'references': 'peeks'}, {'predictions': 'ah-fon-so', 'references': 'af-ohn-soo'}, {'predictions': 'kah-zoo-mee', 'references': 'kah-zoo-mee'}, {'predictions': 'koh-moh-ros', 'references': 'kah-moh-rohz'}, {'predictions': 'mah-see-ray', 'references': 'mah-see-ray'}, {'predictions': 'eh-roh', 'references': 'eh-roh'}, {'predictions': 'see-boo-see-soh', 'references': 'see-boo-see-soh'}, {'predictions': 'boo-koh-lah', 'references': 'boo-koh-lah'}, {'predictions': 'kah-tar-zee-nah', 'references': 'kah-tar-zih-nah'}, {'predictions': 'raht-see-rahk', 'references': 'raht-see-rahk'}, {'predictions': 'hee-nuh', 'references': 'hee-nah'}, {'predictions': 'oh-mah-ree-nee', 'references': 'oh-mah-ree-nee'}, {'predictions': 'hah-nee-yeh', 'references': 'hah-nee-ah'}, {'predictions': 'shuh-nee-koo', 'references': 'shuh-nee-kwuh'}, {'predictions': 'kyung-ho', 'references': 'kyuhng-ho'}, {'predictions': 'sa-mee', 'references': 'sa-mi'}, {'predictions': 'ray-ed', 'references': 'rah-ehd'}, {'predictions': 'ty-reh-keh', 'references': 'ty-reek'}, {'predictions': 'nah-nee', 'references': 'nah-hee'}, {'predictions': 'kleh-ree-thees', 'references': 'kleh-ree-thees'}, {'predictions': 'ahn-vee', 'references': 'uhn-vee'}, {'predictions': 'kha-lee-dah', 'references': 'kah-lee-dah'}, {'predictions': 'bahkht-ee-yor', 'references': 'bahk-tee-yohr'}, {'predictions': 'oh-leh-hee', 'references': 'oh-lehk-see'}, {'predictions': 'loo-see-ah', 'references': 'loo-syah'}, {'predictions': 'nah-bee', 'references': 'nah-bee'}, {'predictions': 'good-luck', 'references': 'good-luhk'}, {'predictions': 'boh-lahn-yohs', 'references': 'boh-lahn-yohs'}, {'predictions': 'shayk', 'references': 'shaykh'}, {'predictions': 'ahb-del-lah', 'references': 'ahb-deh-lah'}, {'predictions': 'mah-rek', 'references': 'mah-rehk'}, {'predictions': 'yah-teen-mah-kee', 'references': 'yah-teen-mah-kee'}, {'predictions': 'mee-kah-tee', 'references': 'mee-kah-tee'}, {'predictions': 'mee-lohs', 'references': 'mee-lohsh'}, {'predictions': 'ay-loh-dee', 'references': 'ay-lo-dee'}, {'predictions': 'proh-tai', 'references': 'proht-eye-ss'}, {'predictions': 'rees-toh', 'references': 'ris-toh'}, {'predictions': 'mahn-zoo-rah', 'references': 'mahn-zoo-rah'}, {'predictions': 'neer', 'references': 'neer'}, {'predictions': 'ee-brah-haa-heem', 'references': 'ib-rah-heem'}, {'predictions': 'kye-slah', 'references': 'kye-slah'}, {'predictions': 'rah-bah-nee', 'references': 'rah-bah-nee'}, {'predictions': 'gheen-goh-nah', 'references': 'gheen-goh-nah'}, {'predictions': 'ahks-el', 'references': 'ahk-sel'}, {'predictions': 'jah-ee-ro', 'references': 'zhah-ee-ro'}, {'predictions': 'hah-med', 'references': 'ham-ehd'}, {'predictions': 'ee-sah-bel-leh', 'references': 'ee-zah-bel'}, {'predictions': 'ee-sah-bel', 'references': 'ee-sah-behl'}, {'predictions': 'zahlm', 'references': 'zahlm'}, {'predictions': 'mahm-boon-doo', 'references': 'mahm-boon-doo'}]\n",
      "2454\n"
     ]
    }
   ],
   "source": [
    "print(f_data[:50])\n",
    "print(len(f_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "331d1c02-30f2-4338-bf54-14de4ec55c92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:0.3482552033765938\n"
     ]
    }
   ],
   "source": [
    "# Calculate BLEU score\n",
    "bleu_score = corpus_bleu(list_of_references=[[ref.split(\"-\")] for ref in references], hypotheses=[pred.split(\"-\") for pred in predictions])\n",
    "print(f\"BLEU Score:{bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45c6de45-6a01-458c-b7fa-b919d404e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge-1': {'r': 0.4612876935615322, 'p': 0.4612876935615322, 'f': 0.4612876912551063}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.4612876935615322, 'p': 0.4612876935615322, 'f': 0.4612876912551063}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate ROUGE score\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(hyps=predictions, refs=references, avg=True)\n",
    "print(f\"ROUGE Scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44513f5-b294-4e77-911c-7c843d720f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e51535-003c-4ffa-8d7d-5fb8c44979f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
